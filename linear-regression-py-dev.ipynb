{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal and Purpose\n",
    "\n",
    "Linear regression is the simplest and arguably most well-known form of machine learning.\n",
    "\n",
    "Linear regression models, as in statistics, are concerned with minimizing error and making the most accurate predictions possible. A neural network can accomplish this by iteratively updating its internal parameters (weights) via a gradient descent algorithm. The Youtube educator 3B1B has a [great visualization](https://www.youtube.com/watch?v=IHZwWFHWa-w) of gradient descent in the context of machine learning models.\n",
    "\n",
    "In this notebook, I will develop a simple single-layer model to \"learn\" how to predict the outcome of a linear function of the form y=mx+b. This excersize may seem trivial or unecessary (\"I can produce a list of solutions for a linear function and plot it with a few lines of code, why do I need a neural network to try and learn how to do the same thing) but this will act as a backbone to build more complex neural networks for much **much** more complicated functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simplest neural net\n",
    "A neural network can be thought of as a function: input, transformation, and output. Therefore, in its most simple representation, a neural network can take a single input, produce an output, and by comparing that output with the known result, can update its internals to better approach the correct output value.\n",
    "\n",
    "![simplest_NN.png](./fig/simplest_NN.png)\n",
    "\n",
    "Here, x is some input number, the input is transformed via our neural network function which has parameters W and b (weight and bias). These parameters are subject to change based on how erroneous the network's output $\\hat{y}$ is compared to the actual value we'd expect from input x.\n",
    "\n",
    "The simple neural network has the following steps:\n",
    "1. Initialize training input `x_train` and output `y_train`. The output here is the expected correct answer.\n",
    "2. Initialize network parameters `W` and `b` . Here the weight array must correspond to the number of inputs. Since we only feed in one input at a time for now, the weights and bias arrays will have shape (1,1). The weight is initialized to a small random number.\n",
    "3. Define our `cost` function. The \"cost\" can be thought of as the error between the expected output and our network's output. \"Cost\" and \"Loss\" are similar, though I believe the Loss function is the averaged error when considering a multitude of simultaneous inputs. We'll showcase this later, for now, each error calculation is refered to as the cost.\n",
    "4. Calculate the components of the gradient of the `cost` function. In this case: $\\frac{\\delta W}{\\delta C}$ and $\\frac{\\delta b}{\\delta C}$.\n",
    "5. Update the network parameters by reducing by a scaled amount of the gradient components. This is *gradient descent*. \n",
    "6. Repeat this process any number of times, called *epochs*. Return the parameters `W` and `b`. \n",
    "7. Use the model's updated parameters on *test data* to determine how accurate the trained model is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the function we want to have our neural network learn to model:\n",
    "# y = 5x + 3\n",
    "def F(x):\n",
    "    f = 5.*x + 3.\n",
    "    return f\n",
    "\n",
    "# 1. initialize x_train, y_train\n",
    "x_train=np.array([2.]).reshape(1,1)\n",
    "y_train=F(x_train)\n",
    "\n",
    "# 2. initialize network parameters\n",
    "W = np.random.randn(1,1)*0.01\n",
    "b = np.zeros(1).reshape(1,1)\n",
    "\n",
    "# Setup an arbitrary function of the form that we are interested in\n",
    "# This is also called the activation function of the neuron.\n",
    "def P(W, b, x):\n",
    "    f = np.dot(W.T,x) + b\n",
    "    return f\n",
    "\n",
    "# 3. Define cost function\n",
    "def C(W,b,x,y):\n",
    "    cost = (y-P(W,b,x))**2\n",
    "    return cost\n",
    "\n",
    "# 4,5,6. Gradient descent, update parameters, repeat\n",
    "def model(W,b,x,y,epochs=5,learning_rate=0.05):\n",
    "    for e in range(epochs):\n",
    "        error=C(W,b,x,y)\n",
    "        dw = 2.*(y-P(W,b,x))*(-1.0*x)\n",
    "        db = -2.*(y-P(W,b,x))\n",
    "        W -= dw*learning_rate\n",
    "        b -= db*learning_rate\n",
    "    return W,b\n",
    "W,b = model(W,b,x_train,y_train,epochs=5,learning_rate=0.05)\n",
    "\n",
    "print(\"y=\",np.mean(W),\"x +\",np.mean(b))\n",
    "\n",
    "# 7. Determine model's accuracy\n",
    "x_test = np.array([5.])\n",
    "y_test = F(x_test)\n",
    "\n",
    "result = W*x_test + b\n",
    "accuracy = 100-np.mean(np.abs(result - y_test))*100\n",
    "print(result, y_test)\n",
    "print(accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y= 5.195246985532606 x + 2.596813878988607\n"
     ]
    }
   ],
   "source": [
    "#x_train=np.arange(0,5)\n",
    "#y_train=F(x_train)\n",
    "\n",
    "# for e in range(5):\n",
    "#     for i,x in enumerate(x_train):\n",
    "#         cost = C(W,b,x,y_train[i])\n",
    "#         dw = 2.*(y_train[i]-P(W,b,x))*(-1.0*x)\n",
    "#         db = -2.*(y_train[i]-P(W,b,x))\n",
    "#         W-=dw*0.05\n",
    "#         b-=db*0.05\n",
    "#         print(cost)\n",
    "#         print(W,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[27.72494767]] [28.]\n",
      "72.49476744582033\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
